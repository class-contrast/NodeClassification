{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb8ed777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
    "                                    OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
    "        super(GNNStack, self).__init__()\n",
    "        conv_model = GAT\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
    "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(args.num_layers-1):\n",
    "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout), \n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.dropout = args.dropout\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        self.emb = emb\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "          \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        if self.emb == True:\n",
    "            return x\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae8abe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(MessagePassing):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, heads = 8,\n",
    "                 negative_slope = 0.2, dropout = 0., **kwargs):\n",
    "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lin_l = None\n",
    "        self.lin_r = None\n",
    "        self.att_l = None\n",
    "        self.att_r = None\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Your code here! \n",
    "        # Define the layers needed for the message functions below.\n",
    "        # self.lin_l is the linear transformation that you apply to embeddings \n",
    "        # BEFORE message passing.\n",
    "        # Pay attention to dimensions of the linear layers, since we're using \n",
    "        # multi-head attention.\n",
    "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
    "\n",
    "        self.lin_l = nn.Linear(self.in_channels, self.out_channels * self.heads)\n",
    "        ############################################################################\n",
    "\n",
    "        self.lin_r = self.lin_l\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Your code here! \n",
    "        # Define the attention parameters \\overrightarrow{a_l/r}^T in the above intro.\n",
    "        # You have to deal with multi-head scenarios.\n",
    "        # Use nn.Parameter instead of nn.Linear\n",
    "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
    "\n",
    "        self.att_l = nn.Parameter(torch.zeros(self.heads, self.out_channels))\n",
    "        self.att_r = nn.Parameter(torch.zeros(self.heads, self.out_channels))\n",
    "        ############################################################################\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.lin_l.weight)\n",
    "        nn.init.xavier_uniform_(self.lin_r.weight)\n",
    "        nn.init.xavier_uniform_(self.att_l)\n",
    "        nn.init.xavier_uniform_(self.att_r)\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "        \n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Your code here! \n",
    "        # Implement message passing, as well as any pre- and post-processing (our update rule).\n",
    "        # 1. First apply linear transformation to node embeddings, and split that \n",
    "        #    into multiple heads. We use the same representations for source and\n",
    "        #    target nodes, but apply different linear weights (W_l and W_r)\n",
    "        # 2. Calculate alpha vectors for central nodes (alpha_l) and neighbor nodes (alpha_r).\n",
    "        # 3. Call propagate function to conduct the message passing. \n",
    "        #    3.1 Remember to pass alpha = (alpha_l, alpha_r) as a parameter.\n",
    "        #    3.2 See there for more information: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
    "        # 4. Transform the output back to the shape of N * d.\n",
    "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
    "\n",
    "        x_l = self.lin_l(x).reshape(-1, H, C)\n",
    "        x_r = self.lin_r(x).reshape(-1, H, C)\n",
    "        alpha_l = self.att_l * x_l\n",
    "        alpha_r = self.att_r * x_r\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), alpha=(alpha_l, alpha_r), size=size)\n",
    "        out = out.reshape(-1, H*C)\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Your code here! \n",
    "        # Implement your message function. Putting the attention in message \n",
    "        # instead of in update is a little tricky.\n",
    "        # 1. Calculate the final attention weights using alpha_i and alpha_j,\n",
    "        #    and apply leaky Relu.\n",
    "        # 2. Calculate softmax over the neighbor nodes for all the nodes. Use \n",
    "        #    torch_geometric.utils.softmax instead of the one in Pytorch.\n",
    "        # 3. Apply dropout to attention weights (alpha).\n",
    "        # 4. Multiply embeddings and attention weights. As a sanity check, the output\n",
    "        #    should be of shape E * H * d.\n",
    "        # 5. ptr (LongTensor, optional): If given, computes the softmax based on\n",
    "        #    sorted inputs in CSR representation. You can simply pass it to softmax.\n",
    "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
    "\n",
    "        alpha = F.leaky_relu(alpha_i + alpha_j, negative_slope=self.negative_slope)\n",
    "        if ptr:\n",
    "            att_weight = F.softmax(alpha_i + alpha_j, ptr)\n",
    "        else:\n",
    "            att_weight = torch_geometric.utils.softmax(alpha_i + alpha_j, index)\n",
    "        att_weight = F.dropout(att_weight, p=self.dropout)\n",
    "        out = att_weight * x_j\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Your code here! \n",
    "        # Implement your aggregate function here.\n",
    "        # See here as how to use torch_scatter.scatter: https://pytorch-scatter.readthedocs.io/en/latest/_modules/torch_scatter/scatter.html\n",
    "        # Pay attention to \"reduce\" parameter is different from that in GraphSage.\n",
    "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
    "\n",
    "        out = torch_scatter.scatter(inputs, index, self.node_dim, dim_size=dim_size, reduce='sum')\n",
    "        ############################################################################\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23bad476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c4a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "61f66090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_node_features,num_classes,train_mask,args):\n",
    "    \n",
    "    #print(\"Node task. test set size:\", np.sum(dataset[0]['train_mask'].numpy()))\n",
    "    #test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(num_node_features, args.hidden_dim, num_classes, \n",
    "                            args)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    for epoch in range(args.epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            pred = pred[train_mask]\n",
    "            label = label[train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "        test_acc = test(test_loader, model)\n",
    "        test_accs.append(test_acc)\n",
    "        if epoch % 1 == 0:\n",
    "          #test_acc = test(test_loader, model)\n",
    "          #test_accs.append(test_acc)\n",
    "            print(\"Epoch \", epoch, \"Loss: \", total_loss, \"Test Acc.: \", test_acc)\n",
    "        #else:\n",
    "         # test_accs.append(test_accs[-1])\n",
    "    return test_accs, losses\n",
    "\n",
    "def test(loader, model, is_validation=True):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            # max(dim=1) returns values, indices tuple; only need indices\n",
    "            pred = model(data).max(dim=1)[1]\n",
    "            label = data.y\n",
    "\n",
    "        mask = data.val_mask if is_validation else data.test_mask\n",
    "        # node classification: only evaluate on nodes in test set\n",
    "        pred = pred[mask]\n",
    "        label = data.y[mask]\n",
    "            \n",
    "        correct += pred.eq(label).sum().item()\n",
    "\n",
    "    total = 0\n",
    "    for data in loader.dataset:\n",
    "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
    "    return correct / total\n",
    "  \n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62f96526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], batch=[2708], ptr=[2])\n",
      "Epoch  0 Loss:  1.951323390007019 Test Acc.:  0.17\n",
      "Epoch  10 Loss:  1.4059704542160034 Test Acc.:  0.364\n",
      "Epoch  20 Loss:  0.3874613046646118 Test Acc.:  0.608\n",
      "Epoch  30 Loss:  0.2111172080039978 Test Acc.:  0.716\n",
      "Epoch  40 Loss:  0.09816200286149979 Test Acc.:  0.71\n",
      "Epoch  50 Loss:  0.08302515000104904 Test Acc.:  0.696\n",
      "Epoch  60 Loss:  0.08721401542425156 Test Acc.:  0.73\n",
      "Epoch  70 Loss:  0.10012725740671158 Test Acc.:  0.712\n",
      "Epoch  80 Loss:  0.06463880836963654 Test Acc.:  0.722\n",
      "Epoch  90 Loss:  0.05699240043759346 Test Acc.:  0.73\n",
      "Maximum accuracy: 0.73\n",
      "Minimum loss: 0.04928265139460564\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='/tmp/cora', name='Cora')\n",
    "test_loader = loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "for data in loader:\n",
    "    print(data)\n",
    "args={'model_type': 'GraphSage', 'dataset': 'cora', 'num_layers': 3, 'heads': 2, \n",
    "         'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 100, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, \n",
    "         'weight_decay': 5e-3, 'lr': 0.01}\n",
    "args = objectview(args)\n",
    "test_accs, losses = train(dataset.num_node_features,dataset.num_classes,data.train_mask,args)\n",
    "print(\"Maximum accuracy: {0}\".format(max(test_accs)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ae01851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x_dom=data.x\n",
    "print(x_dom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d44f8ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>a_0</th>\n",
       "      <th>b_0</th>\n",
       "      <th>a_1</th>\n",
       "      <th>b_1</th>\n",
       "      <th>a_2</th>\n",
       "      <th>b_2</th>\n",
       "      <th>a_3</th>\n",
       "      <th>b_3</th>\n",
       "      <th>a_4</th>\n",
       "      <th>...</th>\n",
       "      <th>S_5</th>\n",
       "      <th>S_6</th>\n",
       "      <th>In_0</th>\n",
       "      <th>In_1</th>\n",
       "      <th>In_2</th>\n",
       "      <th>In_3</th>\n",
       "      <th>In_4</th>\n",
       "      <th>In_5</th>\n",
       "      <th>In_6</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  a_0  b_0  a_1  b_1  a_2  b_2  a_3  b_3  a_4  ...  S_5  S_6  \\\n",
       "0           0    1    0  155    3    5    0    0    0    4  ...    2    1   \n",
       "1           1    0    0    3    1    0    0    0    0    0  ...    7    4   \n",
       "2           2    0    0    0    0    1    0    1    0   40  ...    3    1   \n",
       "3           3    0    0    0    0    0    0    3    0   14  ...    2    5   \n",
       "4           4    0    0    0    0    0    1    0    0    4  ...    2    3   \n",
       "\n",
       "   In_0  In_1  In_2  In_3  In_4  In_5  In_6  Class  \n",
       "0    16    20    20    16    17    12    14      1  \n",
       "1    16    18    18    17    15    16    15      1  \n",
       "2    10    11    12    10    12     7     8      4  \n",
       "3    13    17    19    18    20    14    18      4  \n",
       "4    17    17    19    17    20    15    17      4  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_data = pd.read_csv('Feature_cora.csv')\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96209ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_0</th>\n",
       "      <th>b_0</th>\n",
       "      <th>a_1</th>\n",
       "      <th>b_1</th>\n",
       "      <th>a_2</th>\n",
       "      <th>b_2</th>\n",
       "      <th>a_3</th>\n",
       "      <th>b_3</th>\n",
       "      <th>a_4</th>\n",
       "      <th>b_4</th>\n",
       "      <th>...</th>\n",
       "      <th>S_4</th>\n",
       "      <th>S_5</th>\n",
       "      <th>S_6</th>\n",
       "      <th>In_0</th>\n",
       "      <th>In_1</th>\n",
       "      <th>In_2</th>\n",
       "      <th>In_3</th>\n",
       "      <th>In_4</th>\n",
       "      <th>In_5</th>\n",
       "      <th>In_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   a_0  b_0  a_1  b_1  a_2  b_2  a_3  b_3  a_4  b_4  ...  S_4  S_5  S_6  In_0  \\\n",
       "0    1    0  155    3    5    0    0    0    4    0  ...    3    2    1    16   \n",
       "1    0    0    3    1    0    0    0    0    0    0  ...    6    7    4    16   \n",
       "2    0    0    0    0    1    0    1    0   40    0  ...    4    3    1    10   \n",
       "3    0    0    0    0    0    0    3    0   14    0  ...    8    2    5    13   \n",
       "4    0    0    0    0    0    1    0    0    4    3  ...    8    2    3    17   \n",
       "\n",
       "   In_1  In_2  In_3  In_4  In_5  In_6  \n",
       "0    20    20    16    17    12    14  \n",
       "1    18    18    17    15    16    15  \n",
       "2    11    12    10    12     7     8  \n",
       "3    17    19    18    20    14    18  \n",
       "4    17    19    17    20    15    17  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_c=df_data.drop(['Unnamed: 0','Class'], axis=1)\n",
    "data_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e27f7da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "De=data_c.to_numpy()\n",
    "Fe=np.array(De,dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57ef2eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p_data = open(\"/Users/joshem/PhD Research/Data/cora/cora.cites\")\n",
    "\n",
    "\n",
    "# Create a list from the data\n",
    "myls=list(p_data)\n",
    "\n",
    "# Remove the new line symbole from the list\n",
    "mylist = [line.rstrip('\\n') for line in myls]\n",
    "#print(mylist)\n",
    "#print(graph_ind)\n",
    "#print(graph_level)\n",
    "\n",
    "p_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3b4ae06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708\n"
     ]
    }
   ],
   "source": [
    "mynode=[]\n",
    "Edgelist=[]\n",
    "i=0\n",
    "for d in mylist:\n",
    "    d=mylist[i].split(\"\\t\")\n",
    "    mynode.append(int(d[0]))\n",
    "    mynode.append(int(d[1]))\n",
    "    Edgelist.append([int(d[0]),int(d[1])])\n",
    "    i=i+1\n",
    "#print(mynode)\n",
    "node_ID1=np.unique(mynode)\n",
    "node_ID=list(node_ID1)\n",
    "#print(node_ID)\n",
    "n=len(node_ID)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fef7c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edgelist=[]\n",
    "i=0\n",
    "for d in mylist:\n",
    "    d=mylist[i].split(\"\\t\")\n",
    "    edgelist.append([node_ID.index(int(d[0])),node_ID.index(int(d[1]))])\n",
    "    i=i+1\n",
    "#print(edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "547cb460",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[True, False, True, True, False, False, False, False, False, True, False, True, True, True, True, False, True, True, True, True, True, False, True, True, False, True, False, False, False, False, False, True, False, False, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, False, True, True, True, True, True, True, False, False, False, True, True, True, True, True, False, True, True, True, True, True, False, True, False, True, True, True, False, True, True, True, False, True, True, False, True, True, False, False, True, True, True, True, False, True, True, False, True, True, False, False, False, False, True, True, False, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, True, False, False, False, True, True, False, True, False, False, True, False, True, True, True, False, False, False, True, False, True, True, False, True, True, True, True, False, False, True, True, True, True, True, True, False, True, True, False, True, True, False, True, True, True, False, False, True, False, False, False, False, True, True, True, False, False, True, True, True, True, True, True, False, True, False, True, True, False, True, True, False, True, True, True, True, True, False, True, True, False, False, True, True, False, False, False, False, True, True, True, False, False, True, True, True, True, False, True, True, True, True, False, False, True, True, False, True, True, False, False, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, True, False, True, False, True, True, False, True, True, True, False, False, True, False, False, True, True, False, True, True, True, True, False, True, True, True, True, True, False, True, True, True, True, False, True, False, True, False, True, False, True, True, True, True, False, False, True, True, True, True, False, True, True, False, True, False, False, False, True, True, True, True, True, True, True, True, False, False, True, True, False, False, True, False, True, True, False, False, True, True, False, True, True, True, True, False, False, True, False, False, True, True, True, True, True, True, True, True, True, False, True, True, False, True, False, True, True, False, True, True, False, False, True, True, True, True, False, True, True, False, True, True, False, True, False, True, True, False, True, True, True, False, False, False, True, True, True, False, True, True, True, True, True, False, True, False, False, True, False, False, False, True, False, True, False, False, True, False, False, True, True, True, False, False, True, True, True, False, True, True, True, True, True, True, True, False, True, True, False, False, False, False, False, False, False, True, False, False, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, False, True, False, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, False, True, False, True, True, True, True, False, True, False, True, False, False, False, False, True, False, False, True, False, True, False, True, True, True, True, False, True, True, False, True, False, True, True, True, False, True, True, True, True, True, False, False, True, True, True, False, True, False, True, False, False, True, False, True, True, True, True, False, False, True, True, False, True, True, False, True, False, True, True, True, True, False, True, True, True, True, True, False, False, True, False, False, True, True, True, True, False, True, False, False, True, False, True, True, True, False, True, True, True, False, True, False, False, True, True, False, True, True, True, False, True, True, True, True, False, True, False, False, True, False, True, False, True, True, True, True, True, False, False, False, True, True, True, True, True, True, False, True, True, False, True, False, True, False, False, True, True, False, True, True, False, True, True, True, False, False, True, True, False, True, True, False, True, False, True, True, True, True, True, False, True, True, False, True, True, False, True, True, False, False, True, True, False, True, False, True, True, False, False, True, False, True, True, True, True, False, False, True, False, False, False, True, True, False, True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, False, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, False, False, True, True, True, True, False, True, True, True, True, True, True, True, False, False, True, True, False, True, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, True, True, False, False, True, False, False, True, False, True, False, True, False, True, True, True, True, False, True, True, False, True, True, True, False, True, True, False, False, True, True, True, True, False, True, True, False, True, True, True, False, True, True, False, False, True, True, True, False, True, False, True, False, True, True, False, True, True, False, False, False, True, False, True, True, True, True, False, False, True, True, True, True, False, True, True, False, True, True, True, False, False, True, True, True, False, True, True, True, False, True, False, False, False, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, False, False, False, False, True, True, True, True, True, True, False, True, False, True, True, True, False, True, True, True, False, True, True, True, True, True, True, False, False, True, False, True, False, True, True, True, True, True, False, True, True, False, False, True, True, True, True, True, False, False, False, True, True, True, False, False, False, True, True, False, True, True, False, True, False, True, True, True, False, False, False, False, False, True, False, True, True, True, True, True, False, True, True, True, False, False, True, False, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True, False, True, True, True, False, False, True, True, True, False, True, True, False, True, False, True, False, False, True, True, False, False, True, False, True, True, False, False, True, True, True, False, False, True, True, True, True, False, True, True, True, True, True, True, False, False, True, True, True, True, True, False, False, True, True, False, False, True, True, True, True, False, False, True, False, True, False, False, False, True, True, False, False, False, True, True, False, True, False, False, True, True, True, True, True, True, True, True, True, True, False, True, False, False, False, True, True, False, False, True, True, True, False, False, True, True, True, True, True, False, True, True, True, False, True, False, True, False, True, False, True, True, True, False, True, False, True, False, True, True, True, True, True, True, False, True, True, True, False, True, True, True, False, True, False, True, True, True, False, True, True, True, True, True, True, False, False, False, True, True, False, True, False, True, False, True, True, False, False, False, False, True, True, False, True, True, True, False, True, True, False, False, True, False, False, True, True, False, True, True, True, True, False, True, True, True, True, True, False, False, False, True, True, True, True, False, True, True, True, True, False, True, True, True, False, False, False, False, True, False, False, True, False, True, True, False, False, True, False, True, True, True, True, False, True, True, False, True, True, True, False, False, True, False, False, False, True, True, True, True, True, True, False, True, True, True, True, False, False, False, False, True, True, True, False, True, True, True, False, True, False, True, False, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, False, True, False, True, False, True, True, False, True, True, True, False, True, True, True, True, True, False, True, True, False, False, True, True, False, True, False, True, False, True, False, True, True, False, True, True, False, True, False, False, True, False, False, False, True, True, False, False, True, True, True, False, True, True, False, False, True, True, False, False, True, True, True, True, True, True, True, True, False, True, False, True, False, False, False, True, False, False, False, True, True, False, True, False, False, True, False, True, False, False, True, True, False, True, False, False, False, True, False, False, True, True, False, True, False, False, True, False, True, False, False, False, True, False, False, True, True, False, False, True, False, True, True, True, True, True, False, True, True, False, True, False, False, True, True, False, True, True, True, True, False, False, True, True, False, True, True, True, True, True, True, False, True, True, False, True, True, True, True, False, False, False, False, False, True, True, False, True, True, True, False, True, False, False, True, True, False, False, True, False, True, True, True, True, True, False, True, False, True, True, True, True, True, False, True, True, True, True, False, True, False, True, False, True, False, False, False, True, True, False, True, False, False, True, False, True, False, True, True, True, False, True, True, True, True, True, True, True, False, True, True, False, False, True, True, False, True, False, False, True, False, True, True, True, True, True, False, True, True, True, True, False, True, False, False, False, True, True, False, True, True, True, True, True, False, True, True, True, True, False, False, True, False, True, True, False, False, False, True, False, True, True, False, True, True, True, True, False, True, False, False, True, True, True, True, False, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, True, True, True, True, False, True, True, False, True, False, True, True, True, True, True, False, False, True, True, True, True, False, True, False, True, False, False, False, False, True, False, True, True, True, False, False, False, False, True, True, False, True, False, False, True, True, True, False, True, False, True, True, True, True, False, True, False, False, True, True, True, True, False, False, False, True, True, True, False, True, False, True, True, False, False, False, True, False, True, False, True, True, True, False, False, False, True, False, False, False, True, False, True, True, False, False, True, True, False, True, False, True, True, True, True, True, False, False, False, True, True, True, True, True, True, False, True, True, True, False, False, False, True, False, True, True, True, True, True, True, True, False, True, False, True, True, True, False, True, False, True, False, False, True, True, False, True, True, True, True, False, True, True, True, True, False, True, False, True, True, False, True, True, False, True, False, True, True, False, False, False, False, True, True, False, True, True, True, False, False, True, False, False, True, False, False, True, False, True, False, False, True, False, True, True, True, True, True, True, True, False, True, True, False, False, True, False, False, True, True, True, False, True, True, False, True, True, True, True, False, False, False, False, False, False, True, False, False, True, False, True, False, False, True, True, False, True, True, False, True, True, False, True, True, True, True, False, False, True, False, True, False, False, True, False, True, True, False, True, True, False, False, True, False, False, True, False, True, True, True, True, False, True, True, True, True, True, True, True, False, False, True, False, False, True, True, False, True, True, False, True, True, False, True, True, True, True, True, False, False, True, True, False, True, True, True, True, False, True, True, True, True, True, False, False, True, True, True, True, True, False, True, True, True, True, True, True, True, True, False, True, True, False, False, True, False, False, False, True, True, False, True, True, False, False, False, True, True, True, True, True, True, True, False, True, False, True, True, False, True, True, False, True, False, True, False, True, True, False, True, False, False, False, False, True, True, True, True, True, True, True, False, True, True, False, True, True, False, False, True, False, True, False, True, True, False, True, True, True, False, False, False, True, True, False, False, True, False, True, True, False, True, True, True, True, True, False, False, True, False, True, True, True, True, True, False, True, True, True, False, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, False, False, False, False, True, True, False, False, False, True, True, False, True, False, True, False, False, True, True, True, True, True, True, True, False, True, True, True, True, False, False, True, True, False, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, False, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, True, True, False, True, False, True, True, True, False, True, True, True, True, True, True, False, True, False, False, False, True, True, True, True, False, False, True, True, True, False, False, True, False, True, True, False, True, True, False, False, True, True, True, True, True, True, True, False, True, True, True, True, True, True, False, True, True, False, True, False, True, True, False, False, False, True, True, True, True, False, False, False, False, True, True, True, True, True, True, True, True, True, False, False, False, False, True, True, True, False, False, True, True, False, False, False, True, False, False, True, False, False, True, True, False, False, False, True, True, True, False, True, True, False, True, True, True, True, True, True, False, False, False, True, False, True, True, True, False, False, True, True, False, False, True, True, True, False, False, True, True, True, True, True, True, True, True, False, True, True, False, False, False, True, True, True, True, True, True, True, False, True, True, True, False, True, True, True, False, False, True, True, False, True, True, False, False, False, False, True, True, False, False, True, True, False, True, True, False, True, True, True, True, True, True, True, False, True, True, True, False, False, False, True, True, False, True, False, False, False, True, True, False, True, False, True, True, True, True, False, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, False, True, False, False, True, True, False, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, False, False, False, False, True, False, True, True, True, False, True, False, False, False, True, False, True, False, True, False, True, False, True, False, True, False, False, False, False, True, False, True, True, False, False, True, False, True, False, True, True, True, False, True, True, False, True, True, True, True, False, True, True, True, True, False, False, True, False, False, True, True, False, False, True, False, False, True, False, True, True, True, True, False, True, True, False, False, False, False, True, False, False, True, False, True, False, True, True, True, True, False, True, True, True, True, True, False, True, False, True, False, True, False, True, True, False, True, True, True, False, True, True, False, False, True, False, False, True, True, False, True, False, True, False, True, False, False, True, False, True, False, False, True, True, False, True, True, False, False, True, True, False, True, False, True, True, True, False, True, True, True, False, True, True, True, True, True, False, True, True, False, True, False, False, False, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, False, False, False, True, True, True, True, False, False, False, False, True, True, False, True, False, True, True, True, True, True, True, False, False, True, True, False, True, True, False, False, True, False, True, True, True, False, True, True, False, True, True, False, False, True, True, True, True, False, False, False, False, True, False, False, True, False, False, True, True, False, False, True, True, False, False, True, True, False, False, True, True, False, True, False, False, True, True, True, True, False, True, True, True, False, True, True, False, False, True, True, True, True, True, True, False, True, False, True, True, False, True, True, False, True, False, True, True, False, False, True, True, True, True, True, False, False, True, False, True, False, True, True, True, False, True, True, True, False, True, True, True, False, False, False, False, True, False, True, True, False, False, True, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "60c9f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[]\n",
    "for j in range(2708):\n",
    "    if train[j]==True:\n",
    "        test.append(False)\n",
    "    else:\n",
    "        test.append(True)\n",
    "        j=j+1\n",
    "        \n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e7f731de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.,   0., 155.,  ...,  17.,  12.,  14.],\n",
      "        [  0.,   0.,   3.,  ...,  15.,  16.,  15.],\n",
      "        [  0.,   0.,   0.,  ...,  12.,   7.,   8.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,  15.,  20.,  15.],\n",
      "        [  0.,   0.,   0.,  ...,  13.,  19.,  12.],\n",
      "        [  0.,   0.,   0.,  ...,  13.,  18.,  17.]])\n"
     ]
    }
   ],
   "source": [
    "#val_mask need to be updated\n",
    "import torch\n",
    "edgelist=np.array(edgelist)\n",
    "x = torch.from_numpy(Fe)\n",
    "edge_index=torch.from_numpy(edgelist)\n",
    "Class=df_data['Class'].to_numpy()\n",
    "y=torch.from_numpy(Class)\n",
    "train_mask=torch.from_numpy(np.array(train))\n",
    "test_mask=torch.from_numpy(np.array(test))\n",
    "val_mask=torch.from_numpy(np.array(test))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "51e04d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fd7d7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "dataset = Data(x=x, edge_index=edge_index.t().contiguous(),y=y,train_mask=train_mask,test_mask=test_mask,val_mask=val_mask,name='Cora', num_classes=7,num_node_features=len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bdad439a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[2708, 42], edge_index=[2, 5429], y=[2708], train_mask=[2708], test_mask=[2708], val_mask=[2708], name=[1], num_classes=[1], num_node_features=[1], batch=[2708], ptr=[2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joshem/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "test_loader = loader = DataLoader([dataset], batch_size=1, shuffle=True)\n",
    "for data in loader:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3b43812b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 Loss:  4.517765522003174 Test Acc.:  0.192\n",
      "Epoch  1 Loss:  3.3855526447296143 Test Acc.:  0.348\n",
      "Epoch  2 Loss:  2.229879379272461 Test Acc.:  0.368\n",
      "Epoch  3 Loss:  1.976115345954895 Test Acc.:  0.401\n",
      "Epoch  4 Loss:  1.847705602645874 Test Acc.:  0.438\n",
      "Epoch  5 Loss:  1.7064424753189087 Test Acc.:  0.457\n",
      "Epoch  6 Loss:  1.6396353244781494 Test Acc.:  0.507\n",
      "Epoch  7 Loss:  1.5806668996810913 Test Acc.:  0.496\n",
      "Epoch  8 Loss:  1.566104531288147 Test Acc.:  0.552\n",
      "Epoch  9 Loss:  1.5128272771835327 Test Acc.:  0.563\n",
      "Epoch  10 Loss:  1.4864795207977295 Test Acc.:  0.571\n",
      "Epoch  11 Loss:  1.4148136377334595 Test Acc.:  0.586\n",
      "Epoch  12 Loss:  1.400736689567566 Test Acc.:  0.585\n",
      "Epoch  13 Loss:  1.3949381113052368 Test Acc.:  0.605\n",
      "Epoch  14 Loss:  1.376448392868042 Test Acc.:  0.596\n",
      "Epoch  15 Loss:  1.3895004987716675 Test Acc.:  0.599\n",
      "Epoch  16 Loss:  1.3094711303710938 Test Acc.:  0.622\n",
      "Epoch  17 Loss:  1.290681004524231 Test Acc.:  0.622\n",
      "Epoch  18 Loss:  1.2915173768997192 Test Acc.:  0.615\n",
      "Epoch  19 Loss:  1.238585114479065 Test Acc.:  0.642\n",
      "Epoch  20 Loss:  1.2443846464157104 Test Acc.:  0.63\n",
      "Epoch  21 Loss:  1.2344294786453247 Test Acc.:  0.65\n",
      "Epoch  22 Loss:  1.2564548254013062 Test Acc.:  0.646\n",
      "Epoch  23 Loss:  1.2179404497146606 Test Acc.:  0.652\n",
      "Epoch  24 Loss:  1.2027329206466675 Test Acc.:  0.652\n",
      "Epoch  25 Loss:  1.1695537567138672 Test Acc.:  0.663\n",
      "Epoch  26 Loss:  1.2088068723678589 Test Acc.:  0.639\n",
      "Epoch  27 Loss:  1.1756553649902344 Test Acc.:  0.663\n",
      "Epoch  28 Loss:  1.1685925722122192 Test Acc.:  0.65\n",
      "Epoch  29 Loss:  1.1666895151138306 Test Acc.:  0.675\n",
      "Epoch  30 Loss:  1.1216249465942383 Test Acc.:  0.669\n",
      "Epoch  31 Loss:  1.1419975757598877 Test Acc.:  0.656\n",
      "Epoch  32 Loss:  1.164928674697876 Test Acc.:  0.656\n",
      "Epoch  33 Loss:  1.1464698314666748 Test Acc.:  0.657\n",
      "Epoch  34 Loss:  1.1154696941375732 Test Acc.:  0.669\n",
      "Epoch  35 Loss:  1.1296850442886353 Test Acc.:  0.662\n",
      "Epoch  36 Loss:  1.1196459531784058 Test Acc.:  0.663\n",
      "Epoch  37 Loss:  1.1015909910202026 Test Acc.:  0.657\n",
      "Epoch  38 Loss:  1.0812351703643799 Test Acc.:  0.665\n",
      "Epoch  39 Loss:  1.0887455940246582 Test Acc.:  0.662\n",
      "Epoch  40 Loss:  1.0805493593215942 Test Acc.:  0.665\n",
      "Epoch  41 Loss:  1.0988717079162598 Test Acc.:  0.673\n",
      "Epoch  42 Loss:  1.0799318552017212 Test Acc.:  0.672\n",
      "Epoch  43 Loss:  1.0715997219085693 Test Acc.:  0.654\n",
      "Epoch  44 Loss:  1.096112608909607 Test Acc.:  0.677\n",
      "Epoch  45 Loss:  1.0763945579528809 Test Acc.:  0.666\n",
      "Epoch  46 Loss:  1.0747182369232178 Test Acc.:  0.67\n",
      "Epoch  47 Loss:  1.081916093826294 Test Acc.:  0.658\n",
      "Epoch  48 Loss:  1.0735797882080078 Test Acc.:  0.67\n",
      "Epoch  49 Loss:  1.0637341737747192 Test Acc.:  0.667\n",
      "Epoch  50 Loss:  1.0391989946365356 Test Acc.:  0.669\n",
      "Epoch  51 Loss:  1.0699024200439453 Test Acc.:  0.677\n",
      "Epoch  52 Loss:  1.0683537721633911 Test Acc.:  0.667\n",
      "Epoch  53 Loss:  1.0431935787200928 Test Acc.:  0.671\n",
      "Epoch  54 Loss:  1.0408377647399902 Test Acc.:  0.676\n",
      "Epoch  55 Loss:  1.0508153438568115 Test Acc.:  0.645\n",
      "Epoch  56 Loss:  1.0606611967086792 Test Acc.:  0.673\n",
      "Epoch  57 Loss:  1.032248616218567 Test Acc.:  0.676\n",
      "Epoch  58 Loss:  1.0323559045791626 Test Acc.:  0.66\n",
      "Epoch  59 Loss:  1.083046793937683 Test Acc.:  0.673\n",
      "Epoch  60 Loss:  1.0581932067871094 Test Acc.:  0.673\n",
      "Epoch  61 Loss:  1.0581687688827515 Test Acc.:  0.665\n",
      "Epoch  62 Loss:  1.0183554887771606 Test Acc.:  0.677\n",
      "Epoch  63 Loss:  1.0738967657089233 Test Acc.:  0.674\n",
      "Epoch  64 Loss:  1.0486944913864136 Test Acc.:  0.674\n",
      "Epoch  65 Loss:  1.0462781190872192 Test Acc.:  0.673\n",
      "Epoch  66 Loss:  1.0469391345977783 Test Acc.:  0.681\n",
      "Epoch  67 Loss:  1.060871958732605 Test Acc.:  0.662\n",
      "Epoch  68 Loss:  1.0499261617660522 Test Acc.:  0.664\n",
      "Epoch  69 Loss:  1.0418131351470947 Test Acc.:  0.666\n",
      "Epoch  70 Loss:  1.0343618392944336 Test Acc.:  0.672\n",
      "Epoch  71 Loss:  1.0305970907211304 Test Acc.:  0.654\n",
      "Epoch  72 Loss:  1.0411349534988403 Test Acc.:  0.663\n",
      "Epoch  73 Loss:  1.0292714834213257 Test Acc.:  0.669\n",
      "Epoch  74 Loss:  1.0292994976043701 Test Acc.:  0.673\n",
      "Epoch  75 Loss:  1.081010103225708 Test Acc.:  0.659\n",
      "Epoch  76 Loss:  1.05594801902771 Test Acc.:  0.668\n",
      "Epoch  77 Loss:  1.0038371086120605 Test Acc.:  0.675\n",
      "Epoch  78 Loss:  1.0348213911056519 Test Acc.:  0.668\n",
      "Epoch  79 Loss:  1.0186623334884644 Test Acc.:  0.676\n",
      "Epoch  80 Loss:  1.0220428705215454 Test Acc.:  0.664\n",
      "Epoch  81 Loss:  1.024462342262268 Test Acc.:  0.669\n",
      "Epoch  82 Loss:  1.0070143938064575 Test Acc.:  0.664\n",
      "Epoch  83 Loss:  1.0039582252502441 Test Acc.:  0.673\n",
      "Epoch  84 Loss:  1.0294315814971924 Test Acc.:  0.672\n",
      "Epoch  85 Loss:  1.0270867347717285 Test Acc.:  0.682\n",
      "Epoch  86 Loss:  1.0351725816726685 Test Acc.:  0.672\n",
      "Epoch  87 Loss:  1.0295078754425049 Test Acc.:  0.674\n",
      "Epoch  88 Loss:  0.9937849640846252 Test Acc.:  0.671\n",
      "Epoch  89 Loss:  1.0138128995895386 Test Acc.:  0.667\n",
      "Epoch  90 Loss:  1.0640417337417603 Test Acc.:  0.671\n",
      "Epoch  91 Loss:  1.022991418838501 Test Acc.:  0.673\n",
      "Epoch  92 Loss:  1.0161396265029907 Test Acc.:  0.675\n",
      "Epoch  93 Loss:  1.0120370388031006 Test Acc.:  0.661\n",
      "Epoch  94 Loss:  1.0203005075454712 Test Acc.:  0.682\n",
      "Epoch  95 Loss:  0.9979526996612549 Test Acc.:  0.679\n",
      "Epoch  96 Loss:  1.025077223777771 Test Acc.:  0.667\n",
      "Epoch  97 Loss:  0.9798969030380249 Test Acc.:  0.669\n",
      "Epoch  98 Loss:  1.0188696384429932 Test Acc.:  0.673\n",
      "Epoch  99 Loss:  1.019439697265625 Test Acc.:  0.67\n",
      "Epoch  100 Loss:  0.9956867098808289 Test Acc.:  0.665\n",
      "Epoch  101 Loss:  1.0366096496582031 Test Acc.:  0.672\n",
      "Epoch  102 Loss:  1.0098956823349 Test Acc.:  0.667\n",
      "Epoch  103 Loss:  1.0072821378707886 Test Acc.:  0.67\n",
      "Epoch  104 Loss:  1.0189990997314453 Test Acc.:  0.664\n",
      "Epoch  105 Loss:  1.0281316041946411 Test Acc.:  0.671\n",
      "Epoch  106 Loss:  1.007554292678833 Test Acc.:  0.661\n",
      "Epoch  107 Loss:  1.017000436782837 Test Acc.:  0.671\n",
      "Epoch  108 Loss:  0.9968038201332092 Test Acc.:  0.681\n",
      "Epoch  109 Loss:  1.0425916910171509 Test Acc.:  0.68\n",
      "Epoch  110 Loss:  1.0091086626052856 Test Acc.:  0.666\n",
      "Epoch  111 Loss:  1.034489393234253 Test Acc.:  0.673\n",
      "Epoch  112 Loss:  1.0095881223678589 Test Acc.:  0.667\n",
      "Epoch  113 Loss:  0.9938856363296509 Test Acc.:  0.67\n",
      "Epoch  114 Loss:  0.991064727306366 Test Acc.:  0.655\n",
      "Epoch  115 Loss:  1.0248907804489136 Test Acc.:  0.673\n",
      "Epoch  116 Loss:  1.022868275642395 Test Acc.:  0.673\n",
      "Epoch  117 Loss:  0.9723369479179382 Test Acc.:  0.683\n",
      "Epoch  118 Loss:  1.0346823930740356 Test Acc.:  0.67\n",
      "Epoch  119 Loss:  1.0047523975372314 Test Acc.:  0.673\n",
      "Epoch  120 Loss:  0.9956769943237305 Test Acc.:  0.67\n",
      "Epoch  121 Loss:  1.016784429550171 Test Acc.:  0.667\n",
      "Epoch  122 Loss:  1.0023548603057861 Test Acc.:  0.676\n",
      "Epoch  123 Loss:  1.0133625268936157 Test Acc.:  0.669\n",
      "Epoch  124 Loss:  1.0025025606155396 Test Acc.:  0.667\n",
      "Epoch  125 Loss:  0.9858474731445312 Test Acc.:  0.663\n",
      "Epoch  126 Loss:  1.0045511722564697 Test Acc.:  0.685\n",
      "Epoch  127 Loss:  0.983983039855957 Test Acc.:  0.666\n",
      "Epoch  128 Loss:  1.0076346397399902 Test Acc.:  0.677\n",
      "Epoch  129 Loss:  0.9757012128829956 Test Acc.:  0.677\n",
      "Epoch  130 Loss:  1.0017277002334595 Test Acc.:  0.673\n",
      "Epoch  131 Loss:  0.9803017973899841 Test Acc.:  0.671\n",
      "Epoch  132 Loss:  0.9693666696548462 Test Acc.:  0.68\n",
      "Epoch  133 Loss:  0.9888980984687805 Test Acc.:  0.676\n",
      "Epoch  134 Loss:  0.9871758222579956 Test Acc.:  0.669\n",
      "Epoch  135 Loss:  1.0052870512008667 Test Acc.:  0.673\n",
      "Epoch  136 Loss:  0.9718840718269348 Test Acc.:  0.668\n",
      "Epoch  137 Loss:  0.9968185424804688 Test Acc.:  0.668\n",
      "Epoch  138 Loss:  0.9770849347114563 Test Acc.:  0.681\n",
      "Epoch  139 Loss:  1.0111005306243896 Test Acc.:  0.666\n",
      "Epoch  140 Loss:  0.987899661064148 Test Acc.:  0.682\n",
      "Epoch  141 Loss:  0.9934706091880798 Test Acc.:  0.673\n",
      "Epoch  142 Loss:  0.9751612544059753 Test Acc.:  0.682\n",
      "Epoch  143 Loss:  1.0022205114364624 Test Acc.:  0.678\n",
      "Epoch  144 Loss:  0.9881247282028198 Test Acc.:  0.686\n",
      "Epoch  145 Loss:  0.9967951774597168 Test Acc.:  0.677\n",
      "Epoch  146 Loss:  0.9871769547462463 Test Acc.:  0.673\n",
      "Epoch  147 Loss:  0.9836103916168213 Test Acc.:  0.682\n",
      "Epoch  148 Loss:  0.9821028113365173 Test Acc.:  0.675\n",
      "Epoch  149 Loss:  0.989805281162262 Test Acc.:  0.685\n",
      "Epoch  150 Loss:  0.9645410180091858 Test Acc.:  0.679\n",
      "Epoch  151 Loss:  0.9742110967636108 Test Acc.:  0.667\n",
      "Epoch  152 Loss:  0.9741644859313965 Test Acc.:  0.674\n",
      "Epoch  153 Loss:  0.9570152163505554 Test Acc.:  0.673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  154 Loss:  0.9808917045593262 Test Acc.:  0.679\n",
      "Epoch  155 Loss:  0.9940069317817688 Test Acc.:  0.678\n",
      "Epoch  156 Loss:  1.0053712129592896 Test Acc.:  0.675\n",
      "Epoch  157 Loss:  0.9685268998146057 Test Acc.:  0.685\n",
      "Epoch  158 Loss:  0.9926038980484009 Test Acc.:  0.69\n",
      "Epoch  159 Loss:  0.9987523555755615 Test Acc.:  0.673\n",
      "Epoch  160 Loss:  0.9704938530921936 Test Acc.:  0.672\n",
      "Epoch  161 Loss:  0.9958306550979614 Test Acc.:  0.674\n",
      "Epoch  162 Loss:  0.9867582321166992 Test Acc.:  0.679\n",
      "Epoch  163 Loss:  0.9823467135429382 Test Acc.:  0.668\n",
      "Epoch  164 Loss:  1.0046852827072144 Test Acc.:  0.666\n",
      "Epoch  165 Loss:  0.999979555606842 Test Acc.:  0.677\n",
      "Epoch  166 Loss:  0.9740331768989563 Test Acc.:  0.679\n",
      "Epoch  167 Loss:  0.9778797626495361 Test Acc.:  0.667\n",
      "Epoch  168 Loss:  0.9778170585632324 Test Acc.:  0.668\n",
      "Epoch  169 Loss:  1.001106858253479 Test Acc.:  0.677\n",
      "Epoch  170 Loss:  0.9880629777908325 Test Acc.:  0.67\n",
      "Epoch  171 Loss:  0.9826182723045349 Test Acc.:  0.666\n",
      "Epoch  172 Loss:  0.981067955493927 Test Acc.:  0.68\n",
      "Epoch  173 Loss:  0.99391108751297 Test Acc.:  0.672\n",
      "Epoch  174 Loss:  0.9991042613983154 Test Acc.:  0.674\n",
      "Epoch  175 Loss:  0.9957115054130554 Test Acc.:  0.683\n",
      "Epoch  176 Loss:  0.9851604104042053 Test Acc.:  0.665\n",
      "Epoch  177 Loss:  0.9695829749107361 Test Acc.:  0.678\n",
      "Epoch  178 Loss:  0.9943255186080933 Test Acc.:  0.67\n",
      "Epoch  179 Loss:  0.9768967032432556 Test Acc.:  0.679\n",
      "Epoch  180 Loss:  0.9953639507293701 Test Acc.:  0.681\n",
      "Epoch  181 Loss:  0.9627240300178528 Test Acc.:  0.673\n",
      "Epoch  182 Loss:  0.976036787033081 Test Acc.:  0.682\n",
      "Epoch  183 Loss:  0.9516018629074097 Test Acc.:  0.683\n",
      "Epoch  184 Loss:  0.982296884059906 Test Acc.:  0.675\n",
      "Epoch  185 Loss:  0.9826638698577881 Test Acc.:  0.686\n",
      "Epoch  186 Loss:  1.0094510316848755 Test Acc.:  0.669\n",
      "Epoch  187 Loss:  0.9709852933883667 Test Acc.:  0.662\n",
      "Epoch  188 Loss:  0.988122820854187 Test Acc.:  0.683\n",
      "Epoch  189 Loss:  0.9679654836654663 Test Acc.:  0.675\n",
      "Epoch  190 Loss:  0.9935516715049744 Test Acc.:  0.681\n",
      "Epoch  191 Loss:  0.991208016872406 Test Acc.:  0.673\n",
      "Epoch  192 Loss:  0.9692096710205078 Test Acc.:  0.671\n",
      "Epoch  193 Loss:  0.9924563765525818 Test Acc.:  0.678\n",
      "Epoch  194 Loss:  1.0178947448730469 Test Acc.:  0.666\n",
      "Epoch  195 Loss:  1.0211056470870972 Test Acc.:  0.666\n",
      "Epoch  196 Loss:  0.9968831539154053 Test Acc.:  0.689\n",
      "Epoch  197 Loss:  0.9790118336677551 Test Acc.:  0.667\n",
      "Epoch  198 Loss:  1.0311330556869507 Test Acc.:  0.69\n",
      "Epoch  199 Loss:  0.9886897206306458 Test Acc.:  0.665\n",
      "Maximum accuracy: 0.69\n",
      "Minimum loss: 0.9516018629074097\n"
     ]
    }
   ],
   "source": [
    "args={'model_type': 'GAT', 'dataset': 'cora', 'num_layers': 2, 'heads': 8, \n",
    "         'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.6, 'epochs': 200, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, \n",
    "         'weight_decay': 5e-3, 'lr': 0.01}\n",
    "args = objectview(args)\n",
    "test_accs, losses = train(dataset.num_node_features,dataset.num_classes,data.train_mask,args)\n",
    "print(\"Maximum accuracy: {0}\".format(max(test_accs)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d5d0db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "dataset = Data(x=x_dom, edge_index=edge_index.t().contiguous(),y=y,train_mask=train_mask,test_mask=test_mask,val_mask=val_mask,name='Cora', num_classes=7,num_node_features=len(x_dom[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5c7e7679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[2708, 1433], edge_index=[2, 5429], y=[2708], train_mask=[2708], test_mask=[2708], val_mask=[2708], name=[1], num_classes=[1], num_node_features=[1], batch=[2708], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "test_loader = loader = DataLoader([dataset], batch_size=1, shuffle=True)\n",
    "for data in loader:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bba61746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 Loss:  1.969103455543518 Test Acc.:  0.226\n",
      "Epoch  1 Loss:  1.9266690015792847 Test Acc.:  0.257\n",
      "Epoch  2 Loss:  1.8738350868225098 Test Acc.:  0.257\n",
      "Epoch  3 Loss:  1.8558002710342407 Test Acc.:  0.308\n",
      "Epoch  4 Loss:  1.799263834953308 Test Acc.:  0.352\n",
      "Epoch  5 Loss:  1.781453013420105 Test Acc.:  0.387\n",
      "Epoch  6 Loss:  1.7066009044647217 Test Acc.:  0.393\n",
      "Epoch  7 Loss:  1.6395570039749146 Test Acc.:  0.395\n",
      "Epoch  8 Loss:  1.5602812767028809 Test Acc.:  0.398\n",
      "Epoch  9 Loss:  1.524612307548523 Test Acc.:  0.399\n",
      "Epoch  10 Loss:  1.4993656873703003 Test Acc.:  0.403\n",
      "Epoch  11 Loss:  1.4589827060699463 Test Acc.:  0.415\n",
      "Epoch  12 Loss:  1.4384679794311523 Test Acc.:  0.439\n",
      "Epoch  13 Loss:  1.4211962223052979 Test Acc.:  0.434\n",
      "Epoch  14 Loss:  1.389967679977417 Test Acc.:  0.448\n",
      "Epoch  15 Loss:  1.3508358001708984 Test Acc.:  0.464\n",
      "Epoch  16 Loss:  1.3446868658065796 Test Acc.:  0.45\n",
      "Epoch  17 Loss:  1.4452424049377441 Test Acc.:  0.436\n",
      "Epoch  18 Loss:  1.4335169792175293 Test Acc.:  0.469\n",
      "Epoch  19 Loss:  1.363977074623108 Test Acc.:  0.47\n",
      "Epoch  20 Loss:  1.3089923858642578 Test Acc.:  0.446\n",
      "Epoch  21 Loss:  1.3793553113937378 Test Acc.:  0.483\n",
      "Epoch  22 Loss:  1.2988146543502808 Test Acc.:  0.503\n",
      "Epoch  23 Loss:  1.2661817073822021 Test Acc.:  0.508\n",
      "Epoch  24 Loss:  1.2892169952392578 Test Acc.:  0.514\n",
      "Epoch  25 Loss:  1.2883602380752563 Test Acc.:  0.513\n",
      "Epoch  26 Loss:  1.2451667785644531 Test Acc.:  0.519\n",
      "Epoch  27 Loss:  1.2127922773361206 Test Acc.:  0.504\n",
      "Epoch  28 Loss:  1.2392489910125732 Test Acc.:  0.522\n",
      "Epoch  29 Loss:  1.2031116485595703 Test Acc.:  0.517\n",
      "Epoch  30 Loss:  1.1821324825286865 Test Acc.:  0.543\n",
      "Epoch  31 Loss:  1.1478890180587769 Test Acc.:  0.561\n",
      "Epoch  32 Loss:  1.1755567789077759 Test Acc.:  0.556\n",
      "Epoch  33 Loss:  1.1432262659072876 Test Acc.:  0.551\n",
      "Epoch  34 Loss:  1.1183063983917236 Test Acc.:  0.529\n",
      "Epoch  35 Loss:  1.1288347244262695 Test Acc.:  0.531\n",
      "Epoch  36 Loss:  1.111315131187439 Test Acc.:  0.524\n",
      "Epoch  37 Loss:  1.1288261413574219 Test Acc.:  0.541\n",
      "Epoch  38 Loss:  1.1272631883621216 Test Acc.:  0.547\n",
      "Epoch  39 Loss:  1.1004235744476318 Test Acc.:  0.543\n",
      "Epoch  40 Loss:  1.1130889654159546 Test Acc.:  0.556\n",
      "Epoch  41 Loss:  1.1401090621948242 Test Acc.:  0.564\n",
      "Epoch  42 Loss:  1.0893441438674927 Test Acc.:  0.567\n",
      "Epoch  43 Loss:  1.0959268808364868 Test Acc.:  0.559\n",
      "Epoch  44 Loss:  1.1002386808395386 Test Acc.:  0.559\n",
      "Epoch  45 Loss:  1.1028646230697632 Test Acc.:  0.561\n",
      "Epoch  46 Loss:  1.068893551826477 Test Acc.:  0.571\n",
      "Epoch  47 Loss:  1.0810620784759521 Test Acc.:  0.562\n",
      "Epoch  48 Loss:  1.0851637125015259 Test Acc.:  0.573\n",
      "Epoch  49 Loss:  1.092706561088562 Test Acc.:  0.564\n",
      "Epoch  50 Loss:  1.0626047849655151 Test Acc.:  0.549\n",
      "Epoch  51 Loss:  1.0785435438156128 Test Acc.:  0.551\n",
      "Epoch  52 Loss:  1.053462266921997 Test Acc.:  0.571\n",
      "Epoch  53 Loss:  1.0605449676513672 Test Acc.:  0.558\n",
      "Epoch  54 Loss:  1.065804362297058 Test Acc.:  0.582\n",
      "Epoch  55 Loss:  1.0618107318878174 Test Acc.:  0.558\n",
      "Epoch  56 Loss:  1.0920273065567017 Test Acc.:  0.575\n",
      "Epoch  57 Loss:  1.0505772829055786 Test Acc.:  0.568\n",
      "Epoch  58 Loss:  1.0558561086654663 Test Acc.:  0.567\n",
      "Epoch  59 Loss:  1.0676600933074951 Test Acc.:  0.582\n",
      "Epoch  60 Loss:  1.045407772064209 Test Acc.:  0.562\n",
      "Epoch  61 Loss:  1.0613667964935303 Test Acc.:  0.567\n",
      "Epoch  62 Loss:  1.0630648136138916 Test Acc.:  0.571\n",
      "Epoch  63 Loss:  1.0470402240753174 Test Acc.:  0.584\n",
      "Epoch  64 Loss:  1.0508010387420654 Test Acc.:  0.579\n",
      "Epoch  65 Loss:  1.054241418838501 Test Acc.:  0.571\n",
      "Epoch  66 Loss:  1.0482587814331055 Test Acc.:  0.58\n",
      "Epoch  67 Loss:  1.0399781465530396 Test Acc.:  0.584\n",
      "Epoch  68 Loss:  1.0340851545333862 Test Acc.:  0.588\n",
      "Epoch  69 Loss:  1.033449649810791 Test Acc.:  0.578\n",
      "Epoch  70 Loss:  1.0640130043029785 Test Acc.:  0.56\n",
      "Epoch  71 Loss:  1.024950385093689 Test Acc.:  0.571\n",
      "Epoch  72 Loss:  1.0419459342956543 Test Acc.:  0.58\n",
      "Epoch  73 Loss:  1.0269099473953247 Test Acc.:  0.571\n",
      "Epoch  74 Loss:  1.0245280265808105 Test Acc.:  0.574\n",
      "Epoch  75 Loss:  1.0254970788955688 Test Acc.:  0.584\n",
      "Epoch  76 Loss:  1.044418454170227 Test Acc.:  0.59\n",
      "Epoch  77 Loss:  1.0274865627288818 Test Acc.:  0.567\n",
      "Epoch  78 Loss:  1.0525434017181396 Test Acc.:  0.583\n",
      "Epoch  79 Loss:  1.0284041166305542 Test Acc.:  0.604\n",
      "Epoch  80 Loss:  1.0366389751434326 Test Acc.:  0.58\n",
      "Epoch  81 Loss:  1.0198915004730225 Test Acc.:  0.551\n",
      "Epoch  82 Loss:  1.0298434495925903 Test Acc.:  0.587\n",
      "Epoch  83 Loss:  1.0399653911590576 Test Acc.:  0.59\n",
      "Epoch  84 Loss:  1.0298023223876953 Test Acc.:  0.594\n",
      "Epoch  85 Loss:  1.0144944190979004 Test Acc.:  0.591\n",
      "Epoch  86 Loss:  1.0257987976074219 Test Acc.:  0.594\n",
      "Epoch  87 Loss:  0.9969956278800964 Test Acc.:  0.601\n",
      "Epoch  88 Loss:  0.9946600794792175 Test Acc.:  0.602\n",
      "Epoch  89 Loss:  1.0136268138885498 Test Acc.:  0.608\n",
      "Epoch  90 Loss:  1.0041861534118652 Test Acc.:  0.602\n",
      "Epoch  91 Loss:  0.9930094480514526 Test Acc.:  0.586\n",
      "Epoch  92 Loss:  0.9998108148574829 Test Acc.:  0.594\n",
      "Epoch  93 Loss:  0.9951509237289429 Test Acc.:  0.61\n",
      "Epoch  94 Loss:  0.9889836311340332 Test Acc.:  0.6\n",
      "Epoch  95 Loss:  0.988241970539093 Test Acc.:  0.599\n",
      "Epoch  96 Loss:  0.9824520349502563 Test Acc.:  0.612\n",
      "Epoch  97 Loss:  0.9777077436447144 Test Acc.:  0.605\n",
      "Epoch  98 Loss:  0.9893472790718079 Test Acc.:  0.615\n",
      "Epoch  99 Loss:  0.9793466925621033 Test Acc.:  0.612\n",
      "Epoch  100 Loss:  1.0012221336364746 Test Acc.:  0.611\n",
      "Epoch  101 Loss:  0.9651361107826233 Test Acc.:  0.614\n",
      "Epoch  102 Loss:  0.9854816794395447 Test Acc.:  0.613\n",
      "Epoch  103 Loss:  0.9739955067634583 Test Acc.:  0.633\n",
      "Epoch  104 Loss:  0.9664156436920166 Test Acc.:  0.607\n",
      "Epoch  105 Loss:  0.9840987920761108 Test Acc.:  0.616\n",
      "Epoch  106 Loss:  0.9664691686630249 Test Acc.:  0.608\n",
      "Epoch  107 Loss:  0.9699710607528687 Test Acc.:  0.617\n",
      "Epoch  108 Loss:  0.9659135341644287 Test Acc.:  0.624\n",
      "Epoch  109 Loss:  0.9522530436515808 Test Acc.:  0.633\n",
      "Epoch  110 Loss:  0.9706237316131592 Test Acc.:  0.614\n",
      "Epoch  111 Loss:  0.9802677631378174 Test Acc.:  0.63\n",
      "Epoch  112 Loss:  0.9599965214729309 Test Acc.:  0.613\n",
      "Epoch  113 Loss:  0.9671827554702759 Test Acc.:  0.625\n",
      "Epoch  114 Loss:  0.9681370258331299 Test Acc.:  0.625\n",
      "Epoch  115 Loss:  0.946302056312561 Test Acc.:  0.611\n",
      "Epoch  116 Loss:  0.9543899297714233 Test Acc.:  0.613\n",
      "Epoch  117 Loss:  0.966280460357666 Test Acc.:  0.627\n",
      "Epoch  118 Loss:  0.9427542686462402 Test Acc.:  0.636\n",
      "Epoch  119 Loss:  0.963821291923523 Test Acc.:  0.626\n",
      "Epoch  120 Loss:  0.9372484087944031 Test Acc.:  0.599\n",
      "Epoch  121 Loss:  0.9494454264640808 Test Acc.:  0.593\n",
      "Epoch  122 Loss:  0.9453563094139099 Test Acc.:  0.617\n",
      "Epoch  123 Loss:  0.9977808594703674 Test Acc.:  0.596\n",
      "Epoch  124 Loss:  0.9459075331687927 Test Acc.:  0.614\n",
      "Epoch  125 Loss:  0.9553750157356262 Test Acc.:  0.615\n",
      "Epoch  126 Loss:  0.9464908838272095 Test Acc.:  0.62\n",
      "Epoch  127 Loss:  0.9613806009292603 Test Acc.:  0.649\n",
      "Epoch  128 Loss:  0.9343835711479187 Test Acc.:  0.622\n",
      "Epoch  129 Loss:  0.948112428188324 Test Acc.:  0.618\n",
      "Epoch  130 Loss:  0.9473728537559509 Test Acc.:  0.609\n",
      "Epoch  131 Loss:  0.9364468455314636 Test Acc.:  0.611\n",
      "Epoch  132 Loss:  0.9425573348999023 Test Acc.:  0.617\n",
      "Epoch  133 Loss:  0.9450417160987854 Test Acc.:  0.611\n",
      "Epoch  134 Loss:  0.9947843551635742 Test Acc.:  0.623\n",
      "Epoch  135 Loss:  0.9404723644256592 Test Acc.:  0.625\n",
      "Epoch  136 Loss:  0.9666199684143066 Test Acc.:  0.617\n",
      "Epoch  137 Loss:  0.9421373605728149 Test Acc.:  0.622\n",
      "Epoch  138 Loss:  0.9356421232223511 Test Acc.:  0.638\n",
      "Epoch  139 Loss:  0.9403594136238098 Test Acc.:  0.625\n",
      "Epoch  140 Loss:  0.9459840059280396 Test Acc.:  0.624\n",
      "Epoch  141 Loss:  0.9467275738716125 Test Acc.:  0.622\n",
      "Epoch  142 Loss:  0.9714258313179016 Test Acc.:  0.612\n",
      "Epoch  143 Loss:  0.9522572755813599 Test Acc.:  0.629\n",
      "Epoch  144 Loss:  0.954538106918335 Test Acc.:  0.621\n",
      "Epoch  145 Loss:  0.9265233874320984 Test Acc.:  0.62\n",
      "Epoch  146 Loss:  0.9513434767723083 Test Acc.:  0.623\n",
      "Epoch  147 Loss:  0.9572514295578003 Test Acc.:  0.627\n",
      "Epoch  148 Loss:  1.0125808715820312 Test Acc.:  0.631\n",
      "Epoch  149 Loss:  0.9540677666664124 Test Acc.:  0.627\n",
      "Epoch  150 Loss:  0.9531220197677612 Test Acc.:  0.604\n",
      "Epoch  151 Loss:  0.9523187279701233 Test Acc.:  0.615\n",
      "Epoch  152 Loss:  0.950907289981842 Test Acc.:  0.616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  153 Loss:  0.9400793313980103 Test Acc.:  0.626\n",
      "Epoch  154 Loss:  0.9371069669723511 Test Acc.:  0.624\n",
      "Epoch  155 Loss:  0.9387699961662292 Test Acc.:  0.626\n",
      "Epoch  156 Loss:  0.9662652015686035 Test Acc.:  0.621\n",
      "Epoch  157 Loss:  0.9479339122772217 Test Acc.:  0.63\n",
      "Epoch  158 Loss:  0.9338998794555664 Test Acc.:  0.636\n",
      "Epoch  159 Loss:  0.9299894571304321 Test Acc.:  0.623\n",
      "Epoch  160 Loss:  0.9796943664550781 Test Acc.:  0.625\n",
      "Epoch  161 Loss:  0.9349216818809509 Test Acc.:  0.618\n",
      "Epoch  162 Loss:  0.9309490919113159 Test Acc.:  0.637\n",
      "Epoch  163 Loss:  0.9312840700149536 Test Acc.:  0.612\n",
      "Epoch  164 Loss:  0.9632073044776917 Test Acc.:  0.625\n",
      "Epoch  165 Loss:  0.9421406984329224 Test Acc.:  0.629\n",
      "Epoch  166 Loss:  0.933653712272644 Test Acc.:  0.62\n",
      "Epoch  167 Loss:  0.9237860441207886 Test Acc.:  0.623\n",
      "Epoch  168 Loss:  0.9238949418067932 Test Acc.:  0.62\n",
      "Epoch  169 Loss:  0.9387038946151733 Test Acc.:  0.607\n",
      "Epoch  170 Loss:  0.9390672445297241 Test Acc.:  0.626\n",
      "Epoch  171 Loss:  0.955133855342865 Test Acc.:  0.623\n",
      "Epoch  172 Loss:  0.9350761771202087 Test Acc.:  0.628\n",
      "Epoch  173 Loss:  0.9381521344184875 Test Acc.:  0.636\n",
      "Epoch  174 Loss:  0.9369864463806152 Test Acc.:  0.636\n",
      "Epoch  175 Loss:  0.9261400699615479 Test Acc.:  0.617\n",
      "Epoch  176 Loss:  0.9206162691116333 Test Acc.:  0.615\n",
      "Epoch  177 Loss:  0.9156757593154907 Test Acc.:  0.623\n",
      "Epoch  178 Loss:  0.9250033497810364 Test Acc.:  0.632\n",
      "Epoch  179 Loss:  0.9159473180770874 Test Acc.:  0.629\n",
      "Epoch  180 Loss:  0.9392391443252563 Test Acc.:  0.631\n",
      "Epoch  181 Loss:  0.9347832798957825 Test Acc.:  0.63\n",
      "Epoch  182 Loss:  0.9281830787658691 Test Acc.:  0.619\n",
      "Epoch  183 Loss:  0.9519039988517761 Test Acc.:  0.614\n",
      "Epoch  184 Loss:  0.9430415630340576 Test Acc.:  0.636\n",
      "Epoch  185 Loss:  0.9288341403007507 Test Acc.:  0.631\n",
      "Epoch  186 Loss:  0.9436507821083069 Test Acc.:  0.608\n",
      "Epoch  187 Loss:  0.9232637882232666 Test Acc.:  0.615\n",
      "Epoch  188 Loss:  0.935408890247345 Test Acc.:  0.615\n",
      "Epoch  189 Loss:  0.9344654083251953 Test Acc.:  0.618\n",
      "Epoch  190 Loss:  0.9208460450172424 Test Acc.:  0.622\n",
      "Epoch  191 Loss:  0.9383397698402405 Test Acc.:  0.616\n",
      "Epoch  192 Loss:  0.9693115949630737 Test Acc.:  0.602\n",
      "Epoch  193 Loss:  0.9544175267219543 Test Acc.:  0.609\n",
      "Epoch  194 Loss:  0.9511047005653381 Test Acc.:  0.619\n",
      "Epoch  195 Loss:  0.9234980940818787 Test Acc.:  0.61\n",
      "Epoch  196 Loss:  0.9303252696990967 Test Acc.:  0.631\n",
      "Epoch  197 Loss:  0.931911826133728 Test Acc.:  0.599\n",
      "Epoch  198 Loss:  0.983080267906189 Test Acc.:  0.616\n",
      "Epoch  199 Loss:  0.9477081894874573 Test Acc.:  0.626\n",
      "Epoch  200 Loss:  0.941342294216156 Test Acc.:  0.628\n",
      "Epoch  201 Loss:  0.9561549425125122 Test Acc.:  0.625\n",
      "Epoch  202 Loss:  0.9552344083786011 Test Acc.:  0.611\n",
      "Epoch  203 Loss:  0.9484086632728577 Test Acc.:  0.633\n",
      "Epoch  204 Loss:  0.9421695470809937 Test Acc.:  0.629\n",
      "Epoch  205 Loss:  0.9221999049186707 Test Acc.:  0.628\n",
      "Epoch  206 Loss:  0.959304690361023 Test Acc.:  0.636\n",
      "Epoch  207 Loss:  0.9640904664993286 Test Acc.:  0.631\n",
      "Epoch  208 Loss:  0.9314254522323608 Test Acc.:  0.606\n",
      "Epoch  209 Loss:  0.9336562156677246 Test Acc.:  0.634\n",
      "Epoch  210 Loss:  0.9368547797203064 Test Acc.:  0.631\n",
      "Epoch  211 Loss:  0.9434441328048706 Test Acc.:  0.629\n",
      "Epoch  212 Loss:  0.9223107695579529 Test Acc.:  0.625\n",
      "Epoch  213 Loss:  0.9391261339187622 Test Acc.:  0.618\n",
      "Epoch  214 Loss:  0.9411065578460693 Test Acc.:  0.624\n",
      "Epoch  215 Loss:  0.9541280269622803 Test Acc.:  0.634\n",
      "Epoch  216 Loss:  0.9285898208618164 Test Acc.:  0.621\n",
      "Epoch  217 Loss:  0.9322236180305481 Test Acc.:  0.627\n",
      "Epoch  218 Loss:  0.9201341271400452 Test Acc.:  0.616\n",
      "Epoch  219 Loss:  0.9266707897186279 Test Acc.:  0.625\n",
      "Epoch  220 Loss:  0.9425778985023499 Test Acc.:  0.604\n",
      "Epoch  221 Loss:  0.9242377281188965 Test Acc.:  0.624\n",
      "Epoch  222 Loss:  0.9310809969902039 Test Acc.:  0.629\n",
      "Epoch  223 Loss:  0.9234026670455933 Test Acc.:  0.617\n",
      "Epoch  224 Loss:  0.92596435546875 Test Acc.:  0.627\n",
      "Epoch  225 Loss:  0.9184300899505615 Test Acc.:  0.624\n",
      "Epoch  226 Loss:  0.9377621412277222 Test Acc.:  0.627\n",
      "Epoch  227 Loss:  0.94159334897995 Test Acc.:  0.631\n",
      "Epoch  228 Loss:  0.925803005695343 Test Acc.:  0.62\n",
      "Epoch  229 Loss:  0.9322465658187866 Test Acc.:  0.63\n",
      "Epoch  230 Loss:  0.9245562553405762 Test Acc.:  0.636\n",
      "Epoch  231 Loss:  0.9134098291397095 Test Acc.:  0.644\n",
      "Epoch  232 Loss:  0.9184499382972717 Test Acc.:  0.63\n",
      "Epoch  233 Loss:  0.9235178232192993 Test Acc.:  0.631\n",
      "Epoch  234 Loss:  0.9168606996536255 Test Acc.:  0.631\n",
      "Epoch  235 Loss:  0.913621723651886 Test Acc.:  0.627\n",
      "Epoch  236 Loss:  0.9227088093757629 Test Acc.:  0.621\n",
      "Epoch  237 Loss:  0.9222074151039124 Test Acc.:  0.611\n",
      "Epoch  238 Loss:  0.9357712864875793 Test Acc.:  0.619\n",
      "Epoch  239 Loss:  0.947616696357727 Test Acc.:  0.617\n",
      "Epoch  240 Loss:  0.9162396192550659 Test Acc.:  0.629\n",
      "Epoch  241 Loss:  0.9106506109237671 Test Acc.:  0.624\n",
      "Epoch  242 Loss:  0.9247295260429382 Test Acc.:  0.612\n",
      "Epoch  243 Loss:  0.9311116933822632 Test Acc.:  0.628\n",
      "Epoch  244 Loss:  0.9172635078430176 Test Acc.:  0.629\n",
      "Epoch  245 Loss:  0.9234709143638611 Test Acc.:  0.595\n",
      "Epoch  246 Loss:  0.9469673037528992 Test Acc.:  0.634\n",
      "Epoch  247 Loss:  0.9271127581596375 Test Acc.:  0.624\n",
      "Epoch  248 Loss:  0.9328373670578003 Test Acc.:  0.639\n",
      "Epoch  249 Loss:  0.929406464099884 Test Acc.:  0.621\n",
      "Epoch  250 Loss:  0.9295061230659485 Test Acc.:  0.628\n",
      "Epoch  251 Loss:  0.9193615317344666 Test Acc.:  0.63\n",
      "Epoch  252 Loss:  0.9463071227073669 Test Acc.:  0.628\n",
      "Epoch  253 Loss:  0.9148251414299011 Test Acc.:  0.62\n",
      "Epoch  254 Loss:  0.9090919494628906 Test Acc.:  0.627\n",
      "Epoch  255 Loss:  0.9241188764572144 Test Acc.:  0.633\n",
      "Epoch  256 Loss:  0.885569155216217 Test Acc.:  0.612\n",
      "Epoch  257 Loss:  0.9408571720123291 Test Acc.:  0.63\n",
      "Epoch  258 Loss:  0.9150450825691223 Test Acc.:  0.633\n",
      "Epoch  259 Loss:  0.920066773891449 Test Acc.:  0.635\n",
      "Epoch  260 Loss:  0.9237884879112244 Test Acc.:  0.61\n",
      "Epoch  261 Loss:  0.9175850749015808 Test Acc.:  0.627\n",
      "Epoch  262 Loss:  0.9349092841148376 Test Acc.:  0.625\n",
      "Epoch  263 Loss:  0.9011940360069275 Test Acc.:  0.614\n",
      "Epoch  264 Loss:  0.9296921491622925 Test Acc.:  0.634\n",
      "Epoch  265 Loss:  0.8908131122589111 Test Acc.:  0.627\n",
      "Epoch  266 Loss:  0.9333921074867249 Test Acc.:  0.628\n",
      "Epoch  267 Loss:  0.9056647419929504 Test Acc.:  0.651\n",
      "Epoch  268 Loss:  0.937063992023468 Test Acc.:  0.611\n",
      "Epoch  269 Loss:  0.9254738688468933 Test Acc.:  0.606\n",
      "Epoch  270 Loss:  0.9214723110198975 Test Acc.:  0.626\n",
      "Epoch  271 Loss:  0.9358594417572021 Test Acc.:  0.622\n",
      "Epoch  272 Loss:  0.9060699939727783 Test Acc.:  0.617\n",
      "Epoch  273 Loss:  0.9241124391555786 Test Acc.:  0.619\n",
      "Epoch  274 Loss:  0.9210136532783508 Test Acc.:  0.621\n",
      "Epoch  275 Loss:  0.9431614279747009 Test Acc.:  0.615\n",
      "Epoch  276 Loss:  0.9269390106201172 Test Acc.:  0.615\n",
      "Epoch  277 Loss:  0.9475235342979431 Test Acc.:  0.634\n",
      "Epoch  278 Loss:  0.9296368360519409 Test Acc.:  0.631\n",
      "Epoch  279 Loss:  0.9045255184173584 Test Acc.:  0.622\n",
      "Epoch  280 Loss:  0.9538038969039917 Test Acc.:  0.632\n",
      "Epoch  281 Loss:  0.9110569357872009 Test Acc.:  0.626\n",
      "Epoch  282 Loss:  0.9315338730812073 Test Acc.:  0.64\n",
      "Epoch  283 Loss:  0.925149142742157 Test Acc.:  0.619\n",
      "Epoch  284 Loss:  0.9327269196510315 Test Acc.:  0.625\n",
      "Epoch  285 Loss:  0.917456328868866 Test Acc.:  0.621\n",
      "Epoch  286 Loss:  0.9212750792503357 Test Acc.:  0.63\n",
      "Epoch  287 Loss:  0.9340166449546814 Test Acc.:  0.619\n",
      "Epoch  288 Loss:  0.933000385761261 Test Acc.:  0.631\n",
      "Epoch  289 Loss:  0.9386581778526306 Test Acc.:  0.627\n",
      "Epoch  290 Loss:  0.9433403611183167 Test Acc.:  0.636\n",
      "Epoch  291 Loss:  0.9400550723075867 Test Acc.:  0.633\n",
      "Epoch  292 Loss:  0.9367642998695374 Test Acc.:  0.635\n",
      "Epoch  293 Loss:  0.9066240191459656 Test Acc.:  0.612\n",
      "Epoch  294 Loss:  0.9335076808929443 Test Acc.:  0.626\n",
      "Epoch  295 Loss:  0.916608452796936 Test Acc.:  0.615\n",
      "Epoch  296 Loss:  0.9171475768089294 Test Acc.:  0.619\n",
      "Epoch  297 Loss:  0.9096973538398743 Test Acc.:  0.624\n",
      "Epoch  298 Loss:  0.9457364082336426 Test Acc.:  0.609\n",
      "Epoch  299 Loss:  0.9391576051712036 Test Acc.:  0.623\n",
      "Maximum accuracy: 0.651\n",
      "Minimum loss: 0.885569155216217\n"
     ]
    }
   ],
   "source": [
    "args={'model_type': 'GAT', 'dataset': 'cora', 'num_layers': 2, 'heads': 8, \n",
    "         'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.6, 'epochs': 300, \n",
    "         'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, \n",
    "         'weight_decay': 5e-3, 'lr': 0.01}\n",
    "args = objectview(args)\n",
    "test_accs_dom, losses = train(dataset.num_node_features,dataset.num_classes,data.train_mask,args)\n",
    "print(\"Maximum accuracy: {0}\".format(max(test_accs_dom)))\n",
    "print(\"Minimum loss: {0}\".format(min(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "674fbbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.plot(test_accs_dom,linewidth=3,label='GAT')\n",
    "plt.plot(test_accs,linewidth=3,label='GAT_CC')\n",
    "#plt.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\",fontsize = 17)\n",
    "plt.ylabel(\"Accuracy\",fontsize = 17)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "\n",
    "#plt.ylim(0,1)\n",
    "#plt.legend([\"blue\", \"green\"], loc=\"lower right\")\n",
    "plt.legend(loc=\"lower right\",frameon=False,fontsize = 15)\n",
    "#for pos in ['right', 'top']: \n",
    "    #plt.gca().spines[pos].set_visible(False)\n",
    "plt.savefig(\"CC_GAT.pdf\",bbox_inches=\"tight\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_accs_dom,label='GAT')\n",
    "plt.plot(test_accs,label='CC_GAT')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.savefig(\"CC_GAT.pdf\")\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
